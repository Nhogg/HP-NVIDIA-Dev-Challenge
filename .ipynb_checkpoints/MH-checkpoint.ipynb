{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44c744c-0dca-4c5f-9bea-6d384f34199a",
   "metadata": {},
   "source": [
    "# Mood Mate\n",
    "## Overview\n",
    "Mood Mate is an AI-powered journaling app. Since starting work at an internship this summer, I have not had time during the day to attend therapy. Many people face the same issue, or simply don't have the option to go to therapy. I built Mood Mate to provide a solution to this problem. While it is not a one-to-one replacement for a therapist, it is capable of providing insights and suggestions based on a users journal entries.\n",
    "## What It Does\n",
    "This notebook provides the framework for my classification system. By using a BERT model, we can extract sentiment from text. This sentiment can be fed to another model, Google's Gemini, to make insights even deeper.\n",
    "The model was setup and deployed here. Since it was bundled with AI Studio, I opted to use ML Flow to serve my frontend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5379c4b-c0d7-4b87-8773-df4642448dcb",
   "metadata": {},
   "source": [
    "# Imports\n",
    "For this project, I used PyTorch, as it came prebundled. I attempted to use TensorFlow, but ran into many issues with Keras compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a3fe6-ee18-477d-880a-b5c4cf50a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb055890-dc6e-4b19-88b8-0f0aeafa2b83",
   "metadata": {},
   "source": [
    "# EmotionModelWrapper Class\n",
    "This clas is a custom wrapper for a BERT-based meotion classification model, built using PyTorch and Hugging Face Transformers.\n",
    "## Class Definition\n",
    "We inherit from mlflow.pyfunc.PythonModel which allows seamless integration with MLFlow's model serving and inference APIs.\n",
    "## load_context\n",
    "This method is called after the model succesfully loads. It selects our device (CUDA in this case), loads the BERT tokenizer, loads our pretrained model, and sends the model to the appropriate device.\n",
    "## Predict\n",
    "This method performs inference using our model. We first extract the text data, then tokenize it, run inference on it, and convert the logits to probabilities. Finally, the probabilities are returned as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef5638f-2d66-4c5b-9d0a-9513e5555b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)  # Adjust labels as needed\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        texts = model_input[\"text\"].tolist()\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return probs.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d4236-92cd-4db8-afb4-01e37367ee4a",
   "metadata": {},
   "source": [
    "# MLFlow Model Logging\n",
    "Since MLFlow is integrated with AI Studio, we can use its logging features to keep track of runs, view outputs of the model, and deploy it.\n",
    "This code logs a custom PyFunc model to an experiment.\n",
    "We first setup the experiment. If it already exists, it will log another run. Otherwise, it creates a new one with the given name.\n",
    "Next, we start an MLFlow run and log it.\n",
    "We also define our Conda environment to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "319b1d67-b038-43c7-996c-ee8067cc7ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/09 01:04:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged in run: 1667f57cedb84112affb53b4256a0988\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"Bert-Pretrained\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"emotion_model\",\n",
    "        python_model=EmotionModelWrapper(),\n",
    "        conda_env={\n",
    "            'name': 'mlflow-env',\n",
    "            'channels': ['defaults', 'conda-forge'],\n",
    "            'dependencies': [\n",
    "                'python=3.8',\n",
    "                'pip',\n",
    "                {\n",
    "                    'pip': [\n",
    "                        'torch',\n",
    "                        'transformers',\n",
    "                        'pandas',\n",
    "                        'mlflow',\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    print(f\"Model logged in run: {run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b443a-e804-4a1a-8e0f-688773336298",
   "metadata": {},
   "source": [
    "# Load and Use Logged Model\n",
    "Since we saved our model in the last cell, we can now call and use it again.\n",
    "We provide some test inputs to make sure it is classifying correctly, and perform predictions on it. This allows us to pre-validate outputs before sending the model to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da2de1f-6e16-45a1-a540-57054f09061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10719202 0.16217898 0.22271936 0.11458917 0.19264369 0.20067672]\n",
      " [0.10431828 0.15692514 0.24225228 0.10864547 0.19797336 0.18988553]\n",
      " [0.11122926 0.14448543 0.22541153 0.13491076 0.18686368 0.19709933]]\n"
     ]
    }
   ],
   "source": [
    "run_id = \"1667f57cedb84112affb53b4256a0988\"\n",
    "model_uri = f\"runs:/{run_id}/emotion_model\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "sample_texts = [\n",
    "    \"I feel incredibly sad and alone\",\n",
    "    \"This is a joyful day!\",\n",
    "    \"I am afraid of the future\"\n",
    "]\n",
    "\n",
    "input_df = pd.DataFrame({\"text\": sample_texts})\n",
    "\n",
    "predictions = loaded_model.predict(input_df)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f677770-ed9a-466f-9554-58585dc82e19",
   "metadata": {},
   "source": [
    "# MLFlow combined Model Registration, Promotion, and Inference Pipeline\n",
    "this cell performs a complete workflow with MLFlow. It:\n",
    "1. Registers a logged model\n",
    "2. Promotes it to production\n",
    "3. Loads the production model\n",
    "4. makes test predictions on input text\n",
    "## Define Run ID\n",
    "We first need to define the run ID based on our previously created model.\n",
    "## Model Promotion\n",
    "In order to deploy the model through MLFlow, we have to promote it to production. We also catch errors related to duplicate models and revert back to the previous model.\n",
    "## Model Loading\n",
    "We load the production model using its URI in order to test on sample text.\n",
    "## Test Predictions\n",
    "We provide an array of sample texts, wrap it in a dataframe, and run them through the model. This allows us to spot-check the validity of those predictions before actually sending to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da368aa-5c5c-4f1e-ba0e-12ed9f6bc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'EmotionClassifier' already exists. Creating a new version of this model...\n",
      "Created version '4' of model 'EmotionClassifier'.\n",
      "/tmp/ipykernel_2512/1092276403.py:14: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model registered and promoted to Production: v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:31: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest = client.get_latest_versions(name, None if stage is None else [stage])\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [[0.2941581  0.10023008 0.13040689 0.1924062  0.13009815 0.1527006 ]\n",
      " [0.32240707 0.1026908  0.12823935 0.19151546 0.11668826 0.1384591 ]\n",
      " [0.3327895  0.09975035 0.1295802  0.14300406 0.11933061 0.1755453 ]]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "run_id = \"1667f57cedb84112affb53b4256a0988\"\n",
    "model_name = \"EmotionClassifier\"\n",
    "model_uri = f\"runs:/{run_id}/emotion_model\"\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "try:\n",
    "    result = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=result.version,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True\n",
    "    )\n",
    "    print(f\"Model registered and promoted to Production: v{result.version}\")\n",
    "except MlflowException as e:\n",
    "    print(f\"Model might already be registered or another error occurred: {e}\")\n",
    "\n",
    "prod_model_uri = f\"models:/{model_name}/production\"\n",
    "model = mlflow.pyfunc.load_model(prod_model_uri)\n",
    "\n",
    "sample_texts = [\n",
    "    \"I feel incredibly sad and alone\",\n",
    "    \"This is a joyful day!\",\n",
    "    \"I am afraid of the future\"\n",
    "]\n",
    "input_df = pd.DataFrame({\"text\": sample_texts})\n",
    "\n",
    "predictions = model.predict(input_df)\n",
    "print(\"Predictions:\\n\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
